---
title: "Chapter04"
output:
  github_document: default
  html_notebook: default
---

## 4. "Relationships between words: n-grams and correlations"

many interesting text analyses are based on the relationships between words, whether examining which words tend to follow others immediately, or that tend to co-occur within the same documents.


### 4.1 Tokenizing by n-gram

By seeing how often word X is followed by word Y, we can then build a model of the relationships between them. ... setting `n` to the number of words we wish to capture in each n-gram.
```{r}
library(dplyr)
library(tidytext)
library(janeaustenr)

austen_bigrams <- austen_books() %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)      # `n = 2` : 2 consecutive words : “bigrams”
```

...It is structured as one-token-per-row ... but each token now represents a bigram.


#### 4.1.1 Counting and filtering n-grams

a lot of the most common bigrams are pairs of common (uninteresting) words ... This is a useful time to use tidyr’s `separate()`, which splits a column into multiple based on a delimiter ... we can remove cases where either is a stop-word.
```{r}
library(tidyr)

bigrams_separated <- austen_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# new bigram counts:
bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)
```

...“separate/filter/count/unite” let us find the most common bigrams not containing stop-words.
```{r}
bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")        # `unite()` lets us recombine the columns into one
```



you may be interested in the most common trigrams, which are consecutive sequences of 3 words.
```{r}
austen_books() %>%
  unnest_tokens(trigram, text, token = "ngrams", n = 3) %>%                # n = 3
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word,
         !word3 %in% stop_words$word) %>%
  count(word1, word2, word3, sort = TRUE)
```


#### 4.1.2 Analyzing bigrams

This one-bigram-per-row format is helpful for exploratory analyses of the text. As a simple example, we might be interested in the most common “streets” mentioned in each book:
```{r}
bigrams_filtered %>%
  filter(word2 == "street") %>%
  count(book, word1, sort = TRUE)
```


A bigram can also be treated as a term in a document in the same way that we treated individual words.
```{r}
bigram_tf_idf <- bigrams_united %>%          # from s4.1.1
  count(book, bigram) %>%                    # cols : book, bigram, n
  bind_tf_idf(bigram, book, n) %>%           # cols : book, bigram, n, tf, idf, tf_idf
  arrange(desc(tf_idf))                      # cols : book, bigram, n, tf, idf, tf_idf

bigram_tf_idf
```

...There are advantages and disadvantages to examining the tf-idf of bigrams rather than individual words. Pairs of consecutive words might capture structure that isn’t present when one is just counting single words, and may provide context that makes tokens more understandable ... However, the per-bigram counts are also sparser ... Thus, bigrams can be especially useful when you have a very large text dataset.


#### 4.1.3 Using bigrams to provide context in sentiment analysis

a word’s context can matter nearly as much as its presence. ... Now that we have the data organized into bigrams, it’s easy to tell how often words are preceded by a word like “not”:
```{r}
bigrams_separated %>%                    # from s4.1.1
  filter(word1 == "not") %>%             # cols : book, word1, word2
  count(word1, word2, sort = TRUE)       # cols : word1, word2, n
```

...We could use this to ignore or even reverse their contribution to the sentiment score.


Let’s use the AFINN lexicon for sentiment analysis ... We can then examine the most frequent words that were preceded by “not” and were associated with a sentiment.
```{r}
AFINN <- get_sentiments("afinn")                    # cols : word, score

not_words <- bigrams_separated %>%
  filter(word1 == "not") %>%
  inner_join(AFINN, by = c(word2 = "word")) %>%
  count(word2, score, sort = TRUE) %>%
  ungroup()

not_words
```


It’s worth asking which words contributed the most in the “wrong” direction. To compute that, we can multiply their score by the number of times they appear
```{r}
not_words %>%
  mutate(contribution = n * score) %>%
  arrange(desc(abs(contribution))) %>%
  head(20) %>%
  mutate(word2 = reorder(word2, contribution)) %>%
  ggplot(aes(word2, n * score, fill = n * score > 0)) +        # visualize the result with a bar plot
  geom_col(show.legend = FALSE) +
  xlab("Words preceded by \"not\"") +
  ylab("Sentiment score * number of occurrences") +
  coord_flip()
```






#### 4.1.4 Visualizing a network of bigrams with ggraph
#### 4.1.5 Visualizing bigrams in other texts

### 4.2 Counting and correlating pairs of words with the widyr package
#### 4.2.1 Counting and correlating among sections
#### 4.2.2 Pairwise correlation


### 4.3 Summary




