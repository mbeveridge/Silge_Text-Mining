---
title: "Chapter02"
output:
  github_document: default
  html_notebook: default
---

## 2. "Sentiment Analysis with Tidy Data"
### 2.1 The sentiments dataset
### 2.2 Sentiment analysis with inner join

With data in a tidy format, sentiment analysis can be done as an inner join. ... Let’s look at the words with a joy score from the NRC lexicon. What are the most common joy words in "Emma"?
```{r}
library(janeaustenr)
library(dplyr)
library(stringr)

tidy_books <- austen_books() %>%
  group_by(book) %>%
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]", 
                                                 ignore_case = TRUE)))) %>%
  ungroup() %>%
  unnest_tokens(word, text)   # Lines above are same as §1.3 1st code bloc; this line same as 2nd bloc
```

...Notice that we chose the name `word` for the output column ... performing inner joins and anti-joins is thus easier.


Now that the text is in a tidy format with one word per row, we are ready to do the sentiment analysis.
```{r}
nrc_joy <- get_sentiments("nrc") %>%    # use the NRC lexicon
  filter(sentiment == "joy")            # `filter()` for the joy words

tidy_books %>%
  filter(book == "Emma") %>%            # `filter()` for the words from "Emma"
  inner_join(nrc_joy) %>%               # `inner_join()` to perform the sentiment analysis
  count(word, sort = TRUE)              # `count()` from dplyr for the most common joy words in "Emma"
```



We can also examine how sentiment changes throughout each novel.

count up how many positive and negative words there are in defined sections of each book. ... `index` (using integer division) counts up sections of 80 lines of text.
```{r}
library(tidyr)

jane_austen_sentiment <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%          # sentiment score for each word using Bing
  count(book, index = linenumber %/% 80, sentiment) %>% # `index` tracks where we are in the narrative
  spread(sentiment, n, fill = 0) %>%      # `spread()` sentiment to give -ve and +ve in separate cols
  mutate(sentiment = positive - negative) # lastly calculate a net sentiment (positive - negative)
```


Now we can plot these sentiment scores across the plot trajectory of each novel.
```{r}
library(ggplot2)

ggplot(jane_austen_sentiment, aes(index, sentiment, fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free_x")
```



### 2.3 Comparing the three sentiment dictionaries

Let’s use all three sentiment lexicons and examine how the sentiment changes across the narrative arc of Pride and Prejudice.
```{r}
pride_prejudice <- tidy_books %>% 
  filter(book == "Pride & Prejudice")
```


To find a sentiment score in chunks of text throughout the novel, we will need to use a different pattern for the AFINN lexicon than for the other two.
```{r}
afinn <- pride_prejudice %>% 
  inner_join(get_sentiments("afinn")) %>% 
  group_by(index = linenumber %/% 80) %>%                               # sections of 80 lines
  summarise(sentiment = sum(score)) %>%                                 # sum [score between -5 and 5]
  mutate(method = "AFINN")

bing_and_nrc <- bind_rows(pride_prejudice %>%                           # bind_rows()
                            inner_join(get_sentiments("bing")) %>%
                            mutate(method = "Bing et al."),
                          pride_prejudice %>% 
                            inner_join(get_sentiments("nrc") %>%
                                         filter(sentiment %in% c("positive",      # only these 2 cat's
                                                                 "negative"))) %>%
                            mutate(method = "NRC")) %>%
  count(method, index = linenumber %/% 80, sentiment) %>%               # count [binary: +ve or -ve]
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)                               # net sentiment in sections
```


We now have an estimate of the net sentiment ... for each sentiment lexicon. Let’s bind them together and visualize
```{r}
bind_rows(afinn, bing_and_nrc) %>%
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y")
```

...the NRC sentiment is high, the AFINN sentiment has more variance, the Bing et al. sentiment appears to find longer stretches of similar text, but all three agree roughly on the overall trends




### 2.4 Most common positive and negative words

By implementing count() here with arguments of both word and sentiment, we find out how much each word contributed to each sentiment.
```{r}
bing_word_counts <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()


bing_word_counts %>%                              # This can be shown visually...
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%             # ...and we can pipe straight into ggplot2
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip()
```


the word “miss” is coded as negative but it is used as a title for young, unmarried women in Jane Austen’s works. If it were appropriate for our purposes, we could easily add “miss” to a custom stop-words list using `bind_rows()`.
```{r}
custom_stop_words <- bind_rows(data_frame(word = c("miss"), lexicon = c("custom")), stop_words)
```



### 2.5 Wordclouds

having our data in a tidy format is useful for other plots as well. ... Let’s look at the most common words in Jane Austen’s works as a whole again
```{r}
#install.packages("wordcloud", "~/anaconda3/lib/R/library")      # unable to install (Internet down)
library(wordcloud)                                   # `wordcloud` package, which uses base R graphics

tidy_books %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))
```

Also : [In other functions, such as `comparison.cloud()`, you may need to turn the data frame into a matrix with reshape2’s `acast()`]



### 2.6 Looking at units beyond just words

some sentiment analysis algorithms look beyond only unigrams (i.e. single words) to try to understand the sentiment of a sentence as a whole.

Another option in `unnest_tokens()` is to split into tokens using a regex pattern. We could use this, for example, to split the text of Jane Austen’s novels into a data frame by chapter.
```{r}
austen_chapters <- austen_books() %>%
  group_by(book) %>%
  unnest_tokens(chapter, text, token = "regex", pattern = "Chapter|CHAPTER [\\dIVXLC]") %>%
  ungroup()

austen_chapters %>%                # each row corresponds to one chapter
  group_by(book) %>% 
  summarise(chapters = n())
```

We can use tidy text analysis to ask questions such as what are the most negative chapters in each of Jane Austen’s novels? First, let’s get the list of negative words from the Bing lexicon. [Etc]



### 2.7 Summary
when text data is in a tidy data structure, sentiment analysis can be implemented as an inner join.

